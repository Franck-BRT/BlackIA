import { Dispatch, SetStateAction, MutableRefObject } from 'react';
import type { OllamaMessage } from '@blackia/ollama';
import type { Persona } from '../types/persona';
import type { ChatSettingsData } from '../components/chat/ChatSettings';
import type { MessageMetadata } from './useConversations';

interface UseChatActionsParams {
  // Ã‰tats
  messages: OllamaMessage[];
  setMessages: Dispatch<SetStateAction<OllamaMessage[]>>;
  messageMetadata: Record<number, MessageMetadata>;
  setMessageMetadata: Dispatch<SetStateAction<Record<number, MessageMetadata>>>;
  streamingMessage: string;
  setStreamingMessage: Dispatch<SetStateAction<string>>;
  isGenerating: boolean;
  setIsGenerating: Dispatch<SetStateAction<boolean>>;
  selectedModel: string;
  setRegenerationCounts: Dispatch<SetStateAction<Map<number, number>>>;

  // Refs
  currentStreamIdRef: MutableRefObject<string | null>;
  currentMentionedPersonaIdRef: MutableRefObject<string | undefined>;
  currentMentionedPersonaIdsRef: MutableRefObject<string[] | undefined>;

  // Conversations
  currentConversationId: string | null;
  createConversation: (model: string, title?: string) => any;

  // Personas & Settings
  personas: Persona[];
  currentPersona: Persona | null;
  chatSettings: ChatSettingsData;
  incrementPersonaUsage: (personaId: string) => void;
}

/**
 * Hook pour gÃ©rer toutes les actions principales du chat
 * Send, Stop, Clear, Regenerate, Edit
 */
export function useChatActions({
  messages,
  setMessages,
  messageMetadata,
  setMessageMetadata,
  streamingMessage,
  setStreamingMessage,
  isGenerating,
  setIsGenerating,
  selectedModel,
  setRegenerationCounts,
  currentStreamIdRef,
  currentMentionedPersonaIdRef,
  currentMentionedPersonaIdsRef,
  currentConversationId,
  createConversation,
  personas,
  currentPersona,
  chatSettings,
  incrementPersonaUsage,
}: UseChatActionsParams) {

  // Envoyer un message
  const handleSendMessage = async (
    content: string,
    mentionedPersonaIds?: string[],
    includeMentionFewShots: boolean = false
  ) => {
    console.log('[useChatActions] ðŸ“¥ handleSendMessage reÃ§u:', { mentionedPersonaIds, includeMentionFewShots });

    if (!selectedModel) {
      alert('Veuillez sÃ©lectionner un modÃ¨le');
      return;
    }

    // Stocker les mentionedPersonaIds dans le ref
    currentMentionedPersonaIdsRef.current = mentionedPersonaIds;
    currentMentionedPersonaIdRef.current = mentionedPersonaIds?.[0];

    // CrÃ©er une nouvelle conversation si nÃ©cessaire
    if (!currentConversationId && messages.length === 0) {
      const newConv = createConversation(selectedModel);
      console.log('[useChatActions] âœ¨ Nouvelle conversation crÃ©Ã©e automatiquement:', newConv.id);
    }

    // Ajouter le message de l'utilisateur
    const userMessage: OllamaMessage = {
      role: 'user',
      content,
    };

    const userMessageIndex = messages.length;
    setMessages((prev) => [...prev, userMessage]);

    // Si des personas ont Ã©tÃ© mentionnÃ©s, stocker les mÃ©tadonnÃ©es
    if (mentionedPersonaIds && mentionedPersonaIds.length > 0) {
      setMessageMetadata((prev) => ({
        ...prev,
        [userMessageIndex]: {
          personaId: mentionedPersonaIds[0],
          personaIds: mentionedPersonaIds,
          timestamp: Date.now(),
        },
      }));
      console.log('[useChatActions] ðŸ“ MÃ©tadonnÃ©es ajoutÃ©es pour message utilisateur index', userMessageIndex);
    }

    try {
      console.log('[useChatActions] ðŸ“¤ Envoi du message au backend');

      // DÃ©terminer quels personas utiliser
      const mentionedPersonas = mentionedPersonaIds
        ? mentionedPersonaIds.map(id => personas.find(p => p.id === id)).filter((p): p is Persona => p !== undefined)
        : [];

      const personasToUse = mentionedPersonas.length > 0 ? mentionedPersonas : (currentPersona ? [currentPersona] : []);

      if (mentionedPersonas.length > 0) {
        console.log('[useChatActions] ðŸ“§ Personas mentionnÃ©s (@mention):', mentionedPersonas.map(p => p.name).join(', '));
        mentionedPersonas.forEach(p => incrementPersonaUsage(p.id));
      } else if (currentPersona) {
        incrementPersonaUsage(currentPersona.id);
      }

      // Construire la liste des messages avec le system prompt
      const messagesToSend: OllamaMessage[] = [];
      let systemPromptToUse = '';

      if (personasToUse.length > 0) {
        if (personasToUse.length === 1) {
          systemPromptToUse = personasToUse[0].systemPrompt || '';
        } else {
          const combinedPrompts = personasToUse
            .filter(p => p.systemPrompt)
            .map((p, index) => `[RÃ´le ${index + 1}: ${p.name}]\n${p.systemPrompt}`)
            .join('\n\n---\n\n');

          systemPromptToUse = `Vous devez combiner les perspectives de plusieurs rÃ´les pour rÃ©pondre. Voici les rÃ´les Ã  adopter :\n\n${combinedPrompts}\n\nRÃ©pondez en intÃ©grant les perspectives de tous ces rÃ´les.`;
        }

        // Ajouter les few-shots si demandÃ©
        const shouldIncludeFewShots = mentionedPersonas.length > 0
          ? includeMentionFewShots
          : chatSettings.includeFewShots;

        if (shouldIncludeFewShots) {
          const allFewShots = personasToUse
            .filter(p => p.fewShotExamples && p.fewShotExamples.length > 0)
            .flatMap(p => p.fewShotExamples || []);

          if (allFewShots.length > 0) {
            const fewShotsText = allFewShots
              .map((example) => `Utilisateur: ${example.input}\nAssistant: ${example.output}`)
              .join('\n\n');
            systemPromptToUse += '\n\nExemples:\n' + fewShotsText;
          }
        }
      } else if (chatSettings.systemPrompt.trim()) {
        systemPromptToUse = chatSettings.systemPrompt;
      }

      if (systemPromptToUse) {
        messagesToSend.push({
          role: 'system',
          content: systemPromptToUse,
        });
      }

      messagesToSend.push(...messages, userMessage);

      // DÃ©terminer les paramÃ¨tres Ã  utiliser
      const firstPersona = personasToUse[0];
      const temperature = firstPersona?.temperature ?? chatSettings.temperature;
      const maxTokens = firstPersona?.maxTokens ?? chatSettings.maxTokens;

      // DÃ©terminer le modÃ¨le Ã  utiliser
      let modelToUse = selectedModel;
      if (firstPersona?.model) {
        modelToUse = firstPersona.model;
      }

      // Envoyer la requÃªte de chat avec streaming
      await window.electronAPI.ollama.chatStream({
        model: modelToUse,
        messages: messagesToSend,
        stream: true,
        options: {
          temperature,
          num_ctx: maxTokens,
          top_p: chatSettings.topP,
        },
      });

      console.log('[useChatActions] âœ… Handler chatStream terminÃ©');
    } catch (error: any) {
      console.error('Erreur lors de l\'envoi du message:', error);
      setIsGenerating(false);

      const errorMessage: OllamaMessage = {
        role: 'system',
        content: `âŒ Erreur: ${error.message || 'Erreur inconnue'}`,
      };
      setMessages((prev) => [...prev, errorMessage]);
    }
  };

  // ArrÃªter la gÃ©nÃ©ration
  const handleStop = async () => {
    const streamId = currentStreamIdRef.current;
    if (!streamId) {
      console.log('[useChatActions] âš ï¸ Aucun stream actif Ã  stopper');
      return;
    }

    try {
      console.log('[useChatActions] ðŸ›‘ Demande d\'arrÃªt du stream:', streamId);
      const result = await window.electronAPI.ollama.stopStream(streamId);
      console.log('[useChatActions] âœ… RÃ©ponse stopStream:', result);
    } catch (error: any) {
      console.error('[useChatActions] âŒ Erreur lors du stop:', error);

      // Cleanup local en cas d'erreur
      setIsGenerating(false);
      currentStreamIdRef.current = null;

      if (streamingMessage) {
        const partialMessage: OllamaMessage = {
          role: 'assistant',
          content: streamingMessage + ' [interrompu]',
        };
        setMessages((prev) => [...prev, partialMessage]);
        setStreamingMessage('');
      }
    }
  };

  // Effacer la conversation
  const handleClearChat = () => {
    if (confirm('Voulez-vous vraiment effacer toute la conversation ?')) {
      setMessages([]);
      setMessageMetadata({});
      setStreamingMessage('');
      setIsGenerating(false);
      currentStreamIdRef.current = null;
      currentMentionedPersonaIdRef.current = undefined;
      currentMentionedPersonaIdsRef.current = undefined;
      setRegenerationCounts(new Map());
    }
  };

  // RÃ©gÃ©nÃ©rer la derniÃ¨re rÃ©ponse
  const handleRegenerate = async () => {
    if (isGenerating) {
      return;
    }

    // Trouver le dernier message assistant
    const lastAssistantIndex = messages.findLastIndex((m) => m.role === 'assistant');
    if (lastAssistantIndex === -1) {
      return;
    }

    // Supprimer le dernier message assistant
    const updatedMessages = messages.slice(0, lastAssistantIndex);
    setMessages(updatedMessages);

    // IncrÃ©menter le compteur de rÃ©gÃ©nÃ©ration
    setRegenerationCounts((prev) => {
      const newCounts = new Map(prev);
      const currentCount = newCounts.get(lastAssistantIndex) || 0;
      newCounts.set(lastAssistantIndex, currentCount + 1);
      return newCounts;
    });

    try {
      console.log('[useChatActions] ðŸ”„ RÃ©gÃ©nÃ©ration de la rÃ©ponse');

      // Construire les messages
      const messagesToSend: OllamaMessage[] = [];
      let systemPromptToUse = '';

      if (currentPersona?.systemPrompt) {
        systemPromptToUse = currentPersona.systemPrompt;

        if (chatSettings.includeFewShots && currentPersona.fewShotExamples?.length) {
          const fewShotsText = currentPersona.fewShotExamples
            .map((example) => `Utilisateur: ${example.input}\nAssistant: ${example.output}`)
            .join('\n\n');
          systemPromptToUse += '\n\nExemples:\n' + fewShotsText;
        }
      } else if (chatSettings.systemPrompt.trim()) {
        systemPromptToUse = chatSettings.systemPrompt;
      }

      if (systemPromptToUse) {
        messagesToSend.push({
          role: 'system',
          content: systemPromptToUse,
        });
      }

      messagesToSend.push(...updatedMessages);

      const temperature = currentPersona?.temperature ?? chatSettings.temperature;
      const maxTokens = currentPersona?.maxTokens ?? chatSettings.maxTokens;

      await window.electronAPI.ollama.chatStream({
        model: selectedModel,
        messages: messagesToSend,
        stream: true,
        options: {
          temperature,
          num_ctx: maxTokens,
          top_p: chatSettings.topP,
        },
      });

      console.log('[useChatActions] âœ… RÃ©gÃ©nÃ©ration lancÃ©e');
    } catch (error: any) {
      console.error('Erreur lors de la rÃ©gÃ©nÃ©ration:', error);
      setIsGenerating(false);

      const errorMessage: OllamaMessage = {
        role: 'system',
        content: `âŒ Erreur: ${error.message || 'Erreur inconnue'}`,
      };
      setMessages((prev) => [...prev, errorMessage]);
    }
  };

  // Ã‰diter le dernier message utilisateur
  const handleEditUserMessage = async (newContent: string) => {
    if (isGenerating) {
      return;
    }

    // Trouver le dernier message utilisateur
    const lastUserIndex = messages.findLastIndex((m) => m.role === 'user');
    if (lastUserIndex === -1) {
      return;
    }

    // Mettre Ã  jour le message utilisateur
    const updatedMessages = [...messages];
    updatedMessages[lastUserIndex] = {
      ...updatedMessages[lastUserIndex],
      content: newContent,
    };

    // Supprimer la rÃ©ponse assistant si elle existe
    const lastAssistantIndex = messages.findLastIndex((m) => m.role === 'assistant');
    if (lastAssistantIndex > lastUserIndex) {
      updatedMessages.splice(lastAssistantIndex, 1);
    }

    setMessages(updatedMessages);

    try {
      console.log('[useChatActions] âœï¸ Ã‰dition du message et rÃ©gÃ©nÃ©ration');

      // Construire les messages
      const messagesToSend: OllamaMessage[] = [];
      let systemPromptToUse = '';

      if (currentPersona?.systemPrompt) {
        systemPromptToUse = currentPersona.systemPrompt;

        if (chatSettings.includeFewShots && currentPersona.fewShotExamples?.length) {
          const fewShotsText = currentPersona.fewShotExamples
            .map((example) => `Utilisateur: ${example.input}\nAssistant: ${example.output}`)
            .join('\n\n');
          systemPromptToUse += '\n\nExemples:\n' + fewShotsText;
        }
      } else if (chatSettings.systemPrompt.trim()) {
        systemPromptToUse = chatSettings.systemPrompt;
      }

      if (systemPromptToUse) {
        messagesToSend.push({
          role: 'system',
          content: systemPromptToUse,
        });
      }

      messagesToSend.push(...updatedMessages);

      const temperature = currentPersona?.temperature ?? chatSettings.temperature;
      const maxTokens = currentPersona?.maxTokens ?? chatSettings.maxTokens;

      await window.electronAPI.ollama.chatStream({
        model: selectedModel,
        messages: messagesToSend,
        stream: true,
        options: {
          temperature,
          num_ctx: maxTokens,
          top_p: chatSettings.topP,
        },
      });

      console.log('[useChatActions] âœ… RÃ©gÃ©nÃ©ration lancÃ©e aprÃ¨s Ã©dition');
    } catch (error: any) {
      console.error('Erreur lors de la rÃ©gÃ©nÃ©ration aprÃ¨s Ã©dition:', error);
      setIsGenerating(false);

      const errorMessage: OllamaMessage = {
        role: 'system',
        content: `âŒ Erreur: ${error.message || 'Erreur inconnue'}`,
      };
      setMessages((prev) => [...prev, errorMessage]);
    }
  };

  return {
    handleSendMessage,
    handleStop,
    handleClearChat,
    handleRegenerate,
    handleEditUserMessage,
  };
}
