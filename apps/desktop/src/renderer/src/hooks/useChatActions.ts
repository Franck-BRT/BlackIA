import { Dispatch, SetStateAction, MutableRefObject } from 'react';
import type { OllamaMessage, OllamaTool, OllamaToolCall } from '@blackia/ollama';
import type { Persona } from '../types/persona';
import type { ChatSettingsData } from '../components/chat/ChatSettings';
import type { MessageMetadata } from './useConversations';
import type { WebSearchResponse, WebSearchSettings, WebSearchProviderConfig } from '@blackia/shared';
import type { RAGMetadata } from '../types/attachment';

// Type pour les r√©sultats d'appel d'outil MCP
interface MCPToolCallResult {
  id: string;
  tool: string;
  status: 'pending' | 'running' | 'success' | 'error' | 'cancelled' | 'timeout';
  result?: unknown;
  error?: { code: string; message: string };
  startedAt: number;
  completedAt?: number;
  duration?: number;
}

interface UseChatActionsParams {
  // √âtats
  messages: OllamaMessage[];
  setMessages: Dispatch<SetStateAction<OllamaMessage[]>>;
  messageMetadata: Record<number, MessageMetadata>;
  setMessageMetadata: Dispatch<SetStateAction<Record<number, MessageMetadata>>>;
  streamingMessage: string;
  setStreamingMessage: Dispatch<SetStateAction<string>>;
  isGenerating: boolean;
  setIsGenerating: Dispatch<SetStateAction<boolean>>;
  selectedModel: string;
  setRegenerationCounts: Dispatch<SetStateAction<Map<number, number>>>;

  // Refs
  currentStreamIdRef: MutableRefObject<string | null>;
  currentMentionedPersonaIdRef: MutableRefObject<string | undefined>;
  currentMentionedPersonaIdsRef: MutableRefObject<string[] | undefined>;

  // Conversations
  currentConversationId: string | null;
  createConversation: (model: string, title?: string) => any;

  // Personas & Settings
  personas: Persona[];
  currentPersona: Persona | null;
  chatSettings: ChatSettingsData;
  incrementPersonaUsage: (personaId: string) => void;

  // Web Search
  webSearchEnabled: boolean;
  webSearchSettings: WebSearchSettings;
  setWebSearchResults: Dispatch<SetStateAction<Record<number, WebSearchResponse>>>;
  setIsWebSearching: Dispatch<SetStateAction<boolean>>;

  // MCP Tools
  mcpEnabled: boolean;
  setMcpToolCalls: Dispatch<SetStateAction<OllamaToolCall[]>>;
  setIsMcpExecuting: Dispatch<SetStateAction<boolean>>;
  setMcpError: Dispatch<SetStateAction<string | null>>;
  customToolModels: string[]; // Mod√®les personnalis√©s supportant les tools
}

/**
 * Hook pour g√©rer toutes les actions principales du chat
 * Send, Stop, Clear, Regenerate, Edit
 */
export function useChatActions({
  messages,
  setMessages,
  messageMetadata,
  setMessageMetadata,
  streamingMessage,
  setStreamingMessage,
  isGenerating,
  setIsGenerating,
  selectedModel,
  setRegenerationCounts,
  currentStreamIdRef,
  currentMentionedPersonaIdRef,
  currentMentionedPersonaIdsRef,
  currentConversationId,
  createConversation,
  personas,
  currentPersona,
  chatSettings,
  incrementPersonaUsage,
  webSearchEnabled,
  webSearchSettings,
  setWebSearchResults,
  setIsWebSearching,
  // MCP
  mcpEnabled,
  setMcpToolCalls,
  setIsMcpExecuting,
  setMcpError,
  customToolModels,
}: UseChatActionsParams) {

  // Envoyer un message
  const handleSendMessage = async (
    content: string,
    mentionedPersonaIds?: string[],
    includeMentionFewShots: boolean = false,
    attachmentIds?: string[],
    ragMetadata?: RAGMetadata
  ) => {
    console.log('[useChatActions] üì• handleSendMessage re√ßu:', {
      mentionedPersonaIds,
      includeMentionFewShots,
      attachmentIds,
      hasRagMetadata: !!ragMetadata,
    });

    if (!selectedModel) {
      alert('Veuillez s√©lectionner un mod√®le');
      return;
    }

    // Stocker les mentionedPersonaIds dans le ref
    currentMentionedPersonaIdsRef.current = mentionedPersonaIds;
    currentMentionedPersonaIdRef.current = mentionedPersonaIds?.[0];

    // Cr√©er une nouvelle conversation si n√©cessaire
    if (!currentConversationId && messages.length === 0) {
      const newConv = createConversation(selectedModel);
      console.log('[useChatActions] ‚ú® Nouvelle conversation cr√©√©e automatiquement:', newConv.id);
    }

    // Ajouter le message de l'utilisateur
    const userMessage: OllamaMessage = {
      role: 'user',
      content,
    };

    const userMessageIndex = messages.length;
    setMessages((prev) => [...prev, userMessage]);

    // Stocker les m√©tadonn√©es (personas, attachments, RAG)
    if (mentionedPersonaIds || attachmentIds || ragMetadata) {
      const metadata: MessageMetadata = {
        timestamp: Date.now(),
      };

      if (mentionedPersonaIds && mentionedPersonaIds.length > 0) {
        metadata.personaId = mentionedPersonaIds[0];
        metadata.personaIds = mentionedPersonaIds;
      }

      if (attachmentIds && attachmentIds.length > 0) {
        metadata.attachmentIds = attachmentIds;
      }

      setMessageMetadata((prev) => ({
        ...prev,
        [userMessageIndex]: metadata,
      }));
      console.log('[useChatActions] üìù M√©tadonn√©es ajout√©es pour message utilisateur index', userMessageIndex);
    }

    try {
      console.log('[useChatActions] üì§ Envoi du message au backend');

      // D√©terminer quels personas utiliser
      const mentionedPersonas = mentionedPersonaIds
        ? mentionedPersonaIds.map(id => personas.find(p => p.id === id)).filter((p): p is Persona => p !== undefined)
        : [];

      const personasToUse = mentionedPersonas.length > 0 ? mentionedPersonas : (currentPersona ? [currentPersona] : []);

      if (mentionedPersonas.length > 0) {
        console.log('[useChatActions] üìß Personas mentionn√©s (@mention):', mentionedPersonas.map(p => p.name).join(', '));
        mentionedPersonas.forEach(p => incrementPersonaUsage(p.id));
      } else if (currentPersona) {
        incrementPersonaUsage(currentPersona.id);
      }

      // Construire la liste des messages avec le system prompt
      const messagesToSend: OllamaMessage[] = [];
      let systemPromptToUse = '';

      if (personasToUse.length > 0) {
        if (personasToUse.length === 1) {
          systemPromptToUse = personasToUse[0].systemPrompt || '';
        } else {
          const combinedPrompts = personasToUse
            .filter(p => p.systemPrompt)
            .map((p, index) => `[R√¥le ${index + 1}: ${p.name}]\n${p.systemPrompt}`)
            .join('\n\n---\n\n');

          systemPromptToUse = `Vous devez combiner les perspectives de plusieurs r√¥les pour r√©pondre. Voici les r√¥les √† adopter :\n\n${combinedPrompts}\n\nR√©pondez en int√©grant les perspectives de tous ces r√¥les.`;
        }

        // Ajouter les few-shots si demand√©
        const shouldIncludeFewShots = mentionedPersonas.length > 0
          ? includeMentionFewShots
          : chatSettings.includeFewShots;

        if (shouldIncludeFewShots) {
          const allFewShots = personasToUse
            .filter(p => p.fewShotExamples && p.fewShotExamples.length > 0)
            .flatMap(p => p.fewShotExamples || []);

          if (allFewShots.length > 0) {
            const fewShotsText = allFewShots
              .map((example) => `Utilisateur: ${example.input}\nAssistant: ${example.output}`)
              .join('\n\n');
            systemPromptToUse += '\n\nExemples:\n' + fewShotsText;
          }
        }
      } else if (chatSettings.systemPrompt.trim()) {
        systemPromptToUse = chatSettings.systemPrompt;
      }

      // Ajouter le contenu des attachments au contexte
      if (attachmentIds && attachmentIds.length > 0) {
        try {
          await window.electronAPI.logs.log(
            'info',
            'attachments',
            'R√©cup√©ration du contenu des attachments',
            `Nombre d'attachments: ${attachmentIds.length}`,
            { attachmentIds }
          );

          // R√©cup√©rer les attachments depuis la DB
          const responses = await Promise.all(
            attachmentIds.map(id => window.electronAPI.attachments.getById({ attachmentId: id }))
          );

          // Log des r√©ponses pour debug
          const responseSummary = responses.map(r => ({
            success: r.success,
            hasAttachment: !!r.attachment,
            filename: r.attachment?.originalName,
            hasText: !!r.attachment?.extractedText,
            textLength: r.attachment?.extractedText?.length
          }));

          await window.electronAPI.logs.log(
            'info',
            'attachments',
            'R√©ponses API re√ßues',
            JSON.stringify(responseSummary, null, 2),
            { responsesCount: responses.length }
          );

          // Extraire les attachments des r√©ponses et filtrer les valides avec extractedText
          const validAttachments = responses
            .filter(res => res.success && res.attachment)
            .map(res => res.attachment)
            .filter(att => att && !!att.extractedText);

          await window.electronAPI.logs.log(
            'info',
            'attachments',
            'Attachments valides trouv√©s',
            `${validAttachments.length} fichier(s) avec texte extrait`,
            { validCount: validAttachments.length, totalCount: attachmentIds.length }
          );

          if (validAttachments.length > 0) {
            // Construire le contexte des fichiers
            const filesContext = validAttachments
              .map((attachment, index) => {
                const textLength = attachment.extractedText!.length;
                const truncatedText = textLength > 10000
                  ? attachment.extractedText!.substring(0, 10000) + '\n\n[...texte tronqu√©...]'
                  : attachment.extractedText!;

                // Log de chaque document
                window.electronAPI.logs.log(
                  'info',
                  'attachments',
                  `Document ${index + 1} trait√©`,
                  `Fichier: ${attachment.originalName}`,
                  { filename: attachment.originalName, textLength, truncated: textLength > 10000 }
                );

                return `[Document ${index + 1}: ${attachment.originalName}]\n${truncatedText}`;
              })
              .join('\n\n---\n\n');

            const filesPrompt = `\n\n---\n\nDOCUMENTS JOINTS (${validAttachments.length} fichier${validAttachments.length > 1 ? 's' : ''}):\n\n${filesContext}\n\n---\n\nUtilise ces documents pour r√©pondre √† la question de l'utilisateur.`;
            systemPromptToUse += filesPrompt;

            await window.electronAPI.logs.log(
              'success',
              'attachments',
              'Contexte des fichiers ajout√© au system prompt',
              `Fichiers: ${validAttachments.map(a => a.originalName).join(', ')}`,
              {
                filesCount: validAttachments.length,
                totalChars: filesContext.length,
                systemPromptLength: systemPromptToUse.length
              }
            );
          } else {
            await window.electronAPI.logs.log(
              'warning',
              'attachments',
              'Aucun texte extrait des attachments',
              'Les fichiers ont √©t√© upload√©s mais aucun texte n\'a pu √™tre extrait',
              { responses: responseSummary }
            );
          }
        } catch (error) {
          const errorMsg = error instanceof Error ? error.message : String(error);
          const errorStack = error instanceof Error ? error.stack : 'No stack trace';

          await window.electronAPI.logs.log(
            'error',
            'attachments',
            'Erreur lors de la r√©cup√©ration des attachments',
            errorMsg,
            { stack: errorStack, attachmentIds }
          );
        }
      }

      // Recherche web si activ√©e
      let webSearchData: WebSearchResponse | null = null;
      if (webSearchEnabled && webSearchSettings.enabled) {
        try {
          console.log('[useChatActions] üîç Recherche web activ√©e, recherche en cours...');
          setIsWebSearching(true);

          // Trouver le provider actif
          const activeProvider = webSearchSettings.providers.find(
            (p) => p.id === webSearchSettings.defaultProvider && p.enabled
          );

          if (activeProvider) {
            const searchResult = await window.electronAPI.webSearch.search(
              content, // Utiliser le message utilisateur comme requ√™te
              activeProvider,
              {
                maxResults: webSearchSettings.maxResults,
                language: webSearchSettings.language,
                region: webSearchSettings.region,
                safeSearch: webSearchSettings.safeSearch,
                timeout: webSearchSettings.timeout,
              }
            );

            if (searchResult.success && searchResult.data) {
              webSearchData = searchResult.data;
              console.log('[useChatActions] ‚úÖ Recherche web r√©ussie:', webSearchData.results.length, 'r√©sultats');

              // Construire le contexte web pour le system prompt
              if (webSearchSettings.includeSnippets && webSearchData.results.length > 0) {
                const webContext = webSearchData.results
                  .map((result, index) => {
                    const snippet = result.snippet.substring(0, webSearchSettings.snippetMaxLength);
                    return `[Source ${index + 1}] ${result.title}\nURL: ${result.url}\n${snippet}`;
                  })
                  .join('\n\n');

                const webPrompt = `\n\n---\n\nCONTEXTE WEB (Recherche: "${webSearchData.query}"):\n\n${webContext}\n\n---\n\nUtilise ces informations pour enrichir ta r√©ponse si pertinent.`;
                systemPromptToUse += webPrompt;
              }
            } else {
              console.error('[useChatActions] ‚ùå Erreur recherche web:', searchResult.error);
            }
          } else {
            console.warn('[useChatActions] ‚ö†Ô∏è Aucun provider web actif trouv√©');
          }
        } catch (error) {
          console.error('[useChatActions] ‚ùå Exception recherche web:', error);
        } finally {
          setIsWebSearching(false);
        }
      }

      // D√©terminer les param√®tres √† utiliser
      const firstPersona = personasToUse[0];
      const temperature = firstPersona?.temperature ?? chatSettings.temperature;
      const maxTokens = firstPersona?.maxTokens ?? chatSettings.maxTokens;

      // D√©terminer le mod√®le √† utiliser
      let modelToUse = selectedModel;
      if (firstPersona?.model) {
        modelToUse = firstPersona.model;
      }

      // Mod√®les Ollama qui supportent les function calls / tools (liste par d√©faut)
      const defaultModelsWithToolSupport = [
        'llama3.1', 'llama3.2', 'llama3.3',
        'mistral-nemo', 'mistral', 'mixtral',
        'qwen3', 'qwen2.5', 'qwen2',
        'command-r', 'command-r-plus',
        'firefunction',
        'hermes3', 'hermes2',
        'gpt-oss',
      ];

      // Combiner avec les mod√®les personnalis√©s de l'utilisateur
      const allModelsWithToolSupport = [
        ...defaultModelsWithToolSupport,
        ...customToolModels.map(m => m.toLowerCase()),
      ];

      // V√©rifier si le mod√®le supporte les tools
      const modelBase = modelToUse.split(':')[0].toLowerCase();
      const modelSupportsTools = allModelsWithToolSupport.some(m => modelBase.includes(m));

      // R√©cup√©rer les outils MCP si activ√©s (AVANT de construire les messages)
      let tools: OllamaTool[] | undefined = undefined;
      let disabledToolsInfo = '';
      if (mcpEnabled && modelSupportsTools) {
        try {
          console.log('[useChatActions] üîß R√©cup√©ration des outils MCP avec statut...');
          console.log('[useChatActions] üîß Mod√®le', modelToUse, 'supporte les tools');
          const mcpResult = await window.api.invoke('mcp:getToolsForChatWithStatus');

          if (mcpResult.enabledTools && mcpResult.enabledTools.length > 0) {
            tools = mcpResult.enabledTools as OllamaTool[];
            console.log('[useChatActions] ‚úÖ Outils MCP activ√©s:', mcpResult.enabledTools.length);
          }

          // Si des outils sont d√©sactiv√©s, pr√©parer l'info pour le syst√®me (limit√© √† 10 outils max)
          if (mcpResult.disabledTools && mcpResult.disabledTools.length > 0) {
            console.log('[useChatActions] ‚ö†Ô∏è Outils d√©sactiv√©s:', mcpResult.disabledTools.length);

            // Limiter √† 10 outils pour ne pas surcharger le prompt
            const toolsToShow = mcpResult.disabledTools.slice(0, 10);
            const remainingCount = mcpResult.disabledTools.length - toolsToShow.length;

            const disabledInfo = toolsToShow.map((tool: any) => {
              const missingPerms = tool.missingPermissions?.map((p: any) => p.label).join(', ') || 'permission manquante';
              return `‚Ä¢ ${tool.name}: ${missingPerms}`;
            });

            let infoText = disabledInfo.join('\n');
            if (remainingCount > 0) {
              infoText += `\n‚Ä¢ ... et ${remainingCount} autres outils`;
            }

            disabledToolsInfo = `\n\n[OUTILS NON DISPONIBLES - permissions manquantes]\nPour activer: Outils > Permissions\n${infoText}`;
          }

          if (!tools || tools.length === 0) {
            console.log('[useChatActions] ‚ö†Ô∏è Aucun outil MCP activ√©');
          }
        } catch (error) {
          console.error('[useChatActions] ‚ùå Erreur r√©cup√©ration outils MCP:', error);
          setMcpError(error instanceof Error ? error.message : 'Erreur outils MCP');
          // Continuer sans outils en cas d'erreur
        }
      } else if (mcpEnabled && !modelSupportsTools) {
        // Le mod√®le ne supporte pas les tools
        console.warn('[useChatActions] ‚ö†Ô∏è Mod√®le', modelToUse, 'ne supporte pas les tools');
        disabledToolsInfo = `\n\n[INFO] Le mod√®le ${modelToUse} ne supporte pas les outils MCP. Utilisez un mod√®le compatible (llama3.1, llama3.2, mistral-nemo, qwen2.5, etc.) pour activer les outils.`;
      }

      // Ajouter les infos sur les outils d√©sactiv√©s au system prompt
      if (disabledToolsInfo) {
        systemPromptToUse += disabledToolsInfo;
      }

      // Maintenant construire les messages √† envoyer
      if (systemPromptToUse) {
        messagesToSend.push({
          role: 'system',
          content: systemPromptToUse,
        });
      }

      messagesToSend.push(...messages, userMessage);

      // Construire la requ√™te de chat
      const chatRequest: any = {
        model: modelToUse,
        messages: messagesToSend,
        stream: true,
        options: {
          temperature,
          num_ctx: maxTokens,
          top_p: chatSettings.topP,
        },
      };

      // N'inclure les outils que s'ils existent et ne sont pas vides
      if (tools && tools.length > 0) {
        chatRequest.tools = tools;
        console.log('[useChatActions] üì§ Envoi avec', tools.length, 'outils');

        // Ajouter des instructions pour que le mod√®le utilise les outils
        const toolsList = tools.map(t => `‚Ä¢ ${t.function.name}: ${t.function.description || 'Aucune description'}`).join('\n');
        const toolInstructions = `\n\n[OUTILS DISPONIBLES]\nVous avez acc√®s aux outils suivants. IMPORTANT: Utilisez ces outils directement plut√¥t que d'expliquer comment faire manuellement.\n\n${toolsList}\n\nQuand l'utilisateur demande une action que vous pouvez accomplir avec un outil, appelez l'outil correspondant au lieu de donner des instructions manuelles.`;

        // Injecter les instructions dans le premier message syst√®me ou en cr√©er un
        if (messagesToSend.length > 0 && messagesToSend[0].role === 'system') {
          messagesToSend[0].content += toolInstructions;
        } else {
          messagesToSend.unshift({
            role: 'system',
            content: toolInstructions.trim(),
          });
        }

        // Mettre √† jour la requ√™te avec les messages modifi√©s
        chatRequest.messages = messagesToSend;
      } else {
        console.log('[useChatActions] üì§ Envoi sans outils');
      }

      // Envoyer la requ√™te de chat avec streaming
      await window.electronAPI.ollama.chatStream(chatRequest);

      // Sauvegarder les r√©sultats de recherche web pour l'affichage
      if (webSearchData && webSearchData.results.length > 0) {
        const assistantMessageIndex = messages.length + 1; // Index du prochain message assistant
        setWebSearchResults((prev) => ({
          ...prev,
          [assistantMessageIndex]: webSearchData,
        }));
        console.log('[useChatActions] üíæ R√©sultats web sauvegard√©s pour message index', assistantMessageIndex);
      }

      // Sauvegarder les m√©tadonn√©es RAG pour l'affichage
      if (ragMetadata && ragMetadata.enabled) {
        const assistantMessageIndex = messages.length + 1; // Index du prochain message assistant
        setMessageMetadata((prev) => ({
          ...prev,
          [assistantMessageIndex]: {
            ...prev[assistantMessageIndex],
            ragMetadata,
            timestamp: Date.now(),
          },
        }));
        console.log('[useChatActions] üíæ M√©tadonn√©es RAG sauvegard√©es pour message index', assistantMessageIndex);
      }

      console.log('[useChatActions] ‚úÖ Handler chatStream termin√©');
    } catch (error: any) {
      console.error('Erreur lors de l\'envoi du message:', error);
      setIsGenerating(false);

      const errorMessage: OllamaMessage = {
        role: 'system',
        content: `‚ùå Erreur: ${error.message || 'Erreur inconnue'}`,
      };
      setMessages((prev) => [...prev, errorMessage]);
    }
  };

  // Arr√™ter la g√©n√©ration
  const handleStop = async () => {
    const streamId = currentStreamIdRef.current;
    if (!streamId) {
      console.log('[useChatActions] ‚ö†Ô∏è Aucun stream actif √† stopper');
      return;
    }

    try {
      console.log('[useChatActions] üõë Demande d\'arr√™t du stream:', streamId);
      const result = await window.electronAPI.ollama.stopStream(streamId);
      console.log('[useChatActions] ‚úÖ R√©ponse stopStream:', result);
    } catch (error: any) {
      console.error('[useChatActions] ‚ùå Erreur lors du stop:', error);

      // Cleanup local en cas d'erreur
      setIsGenerating(false);
      currentStreamIdRef.current = null;

      if (streamingMessage) {
        const partialMessage: OllamaMessage = {
          role: 'assistant',
          content: streamingMessage + ' [interrompu]',
        };
        setMessages((prev) => [...prev, partialMessage]);
        setStreamingMessage('');
      }
    }
  };

  // Effacer la conversation
  const handleClearChat = () => {
    if (confirm('Voulez-vous vraiment effacer toute la conversation ?')) {
      setMessages([]);
      setMessageMetadata({});
      setStreamingMessage('');
      setIsGenerating(false);
      currentStreamIdRef.current = null;
      currentMentionedPersonaIdRef.current = undefined;
      currentMentionedPersonaIdsRef.current = undefined;
      setRegenerationCounts(new Map());
    }
  };

  // R√©g√©n√©rer la derni√®re r√©ponse
  const handleRegenerate = async () => {
    if (isGenerating) {
      return;
    }

    // Trouver le dernier message assistant
    const lastAssistantIndex = messages.findLastIndex((m) => m.role === 'assistant');
    if (lastAssistantIndex === -1) {
      return;
    }

    // Supprimer le dernier message assistant
    const updatedMessages = messages.slice(0, lastAssistantIndex);
    setMessages(updatedMessages);

    // Incr√©menter le compteur de r√©g√©n√©ration
    setRegenerationCounts((prev) => {
      const newCounts = new Map(prev);
      const currentCount = newCounts.get(lastAssistantIndex) || 0;
      newCounts.set(lastAssistantIndex, currentCount + 1);
      return newCounts;
    });

    try {
      console.log('[useChatActions] üîÑ R√©g√©n√©ration de la r√©ponse');

      // Construire les messages
      const messagesToSend: OllamaMessage[] = [];
      let systemPromptToUse = '';

      if (currentPersona?.systemPrompt) {
        systemPromptToUse = currentPersona.systemPrompt;

        if (chatSettings.includeFewShots && currentPersona.fewShotExamples?.length) {
          const fewShotsText = currentPersona.fewShotExamples
            .map((example) => `Utilisateur: ${example.input}\nAssistant: ${example.output}`)
            .join('\n\n');
          systemPromptToUse += '\n\nExemples:\n' + fewShotsText;
        }
      } else if (chatSettings.systemPrompt.trim()) {
        systemPromptToUse = chatSettings.systemPrompt;
      }

      if (systemPromptToUse) {
        messagesToSend.push({
          role: 'system',
          content: systemPromptToUse,
        });
      }

      messagesToSend.push(...updatedMessages);

      const temperature = currentPersona?.temperature ?? chatSettings.temperature;
      const maxTokens = currentPersona?.maxTokens ?? chatSettings.maxTokens;

      await window.electronAPI.ollama.chatStream({
        model: selectedModel,
        messages: messagesToSend,
        stream: true,
        options: {
          temperature,
          num_ctx: maxTokens,
          top_p: chatSettings.topP,
        },
      });

      console.log('[useChatActions] ‚úÖ R√©g√©n√©ration lanc√©e');
    } catch (error: any) {
      console.error('Erreur lors de la r√©g√©n√©ration:', error);
      setIsGenerating(false);

      const errorMessage: OllamaMessage = {
        role: 'system',
        content: `‚ùå Erreur: ${error.message || 'Erreur inconnue'}`,
      };
      setMessages((prev) => [...prev, errorMessage]);
    }
  };

  // √âditer le dernier message utilisateur
  const handleEditUserMessage = async (newContent: string) => {
    if (isGenerating) {
      return;
    }

    // Trouver le dernier message utilisateur
    const lastUserIndex = messages.findLastIndex((m) => m.role === 'user');
    if (lastUserIndex === -1) {
      return;
    }

    // Mettre √† jour le message utilisateur
    const updatedMessages = [...messages];
    updatedMessages[lastUserIndex] = {
      ...updatedMessages[lastUserIndex],
      content: newContent,
    };

    // Supprimer la r√©ponse assistant si elle existe
    const lastAssistantIndex = messages.findLastIndex((m) => m.role === 'assistant');
    if (lastAssistantIndex > lastUserIndex) {
      updatedMessages.splice(lastAssistantIndex, 1);
    }

    setMessages(updatedMessages);

    try {
      console.log('[useChatActions] ‚úèÔ∏è √âdition du message et r√©g√©n√©ration');

      // Construire les messages
      const messagesToSend: OllamaMessage[] = [];
      let systemPromptToUse = '';

      if (currentPersona?.systemPrompt) {
        systemPromptToUse = currentPersona.systemPrompt;

        if (chatSettings.includeFewShots && currentPersona.fewShotExamples?.length) {
          const fewShotsText = currentPersona.fewShotExamples
            .map((example) => `Utilisateur: ${example.input}\nAssistant: ${example.output}`)
            .join('\n\n');
          systemPromptToUse += '\n\nExemples:\n' + fewShotsText;
        }
      } else if (chatSettings.systemPrompt.trim()) {
        systemPromptToUse = chatSettings.systemPrompt;
      }

      if (systemPromptToUse) {
        messagesToSend.push({
          role: 'system',
          content: systemPromptToUse,
        });
      }

      messagesToSend.push(...updatedMessages);

      const temperature = currentPersona?.temperature ?? chatSettings.temperature;
      const maxTokens = currentPersona?.maxTokens ?? chatSettings.maxTokens;

      await window.electronAPI.ollama.chatStream({
        model: selectedModel,
        messages: messagesToSend,
        stream: true,
        options: {
          temperature,
          num_ctx: maxTokens,
          top_p: chatSettings.topP,
        },
      });

      console.log('[useChatActions] ‚úÖ R√©g√©n√©ration lanc√©e apr√®s √©dition');
    } catch (error: any) {
      console.error('Erreur lors de la r√©g√©n√©ration apr√®s √©dition:', error);
      setIsGenerating(false);

      const errorMessage: OllamaMessage = {
        role: 'system',
        content: `‚ùå Erreur: ${error.message || 'Erreur inconnue'}`,
      };
      setMessages((prev) => [...prev, errorMessage]);
    }
  };

  return {
    handleSendMessage,
    handleStop,
    handleClearChat,
    handleRegenerate,
    handleEditUserMessage,
  };
}
