# BlackIA Python Services

Python backend pour le systÃ¨me RAG (Retrieval Augmented Generation) de BlackIA.

## ğŸ¯ FonctionnalitÃ©s

- **Text RAG** : Embeddings textuels via Ollama (nomic-embed-text)
- **Vision RAG** : Embeddings visuels via MLX-VLM (Qwen2-VL adapter)
- **Late Interaction Matching** : Recherche multi-vecteurs style ColPali
- **Document Processing** : Conversion PDF â†’ Images pour Vision RAG

## ğŸ“‹ PrÃ©requis

- **Python 3.11+**
- **Apple Silicon** (M1/M2/M3/M4) pour MLX
- **macOS 13+** recommandÃ©
- **16GB RAM minimum** pour Vision RAG
- **32GB RAM** recommandÃ© pour Qwen2-VL-7B

## ğŸš€ Installation

### 1. Setup automatique

```bash
cd apps/desktop/src/python
./setup.sh
```

### 2. Installation manuelle

```bash
# CrÃ©er virtual environment
python3 -m venv venv

# Activer venv
source venv/bin/activate

# Installer dÃ©pendances
pip install -r requirements.txt
```

### 3. VÃ©rifier l'installation

```bash
# Test MLX
python -c "import mlx.core as mx; print(mx.ones((2,2)))"

# Test MLX-VLM
python -c "import mlx_vlm; print('MLX-VLM OK')"

# Test LanceDB
python -c "import lancedb; print('LanceDB OK')"
```

## ğŸ“ Structure

```
python/
â”œâ”€â”€ requirements.txt          # DÃ©pendances Python
â”œâ”€â”€ setup.sh                  # Script d'installation
â”œâ”€â”€ __init__.py
â”œâ”€â”€ text_rag/                 # TEXT RAG module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ollama_embeddings.py  # Ollama integration
â”œâ”€â”€ vision_rag/               # VISION RAG module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mlx_vision_embedder.py   # MLX-VLM wrapper
â”‚   â”œâ”€â”€ late_interaction.py      # MaxSim matching
â”‚   â””â”€â”€ document_processor.py    # PDF â†’ Images
â””â”€â”€ utils/                    # Utilities
    â”œâ”€â”€ __init__.py
    â””â”€â”€ vector_store_utils.py    # LanceDB helpers
```

## ğŸ”§ Utilisation

### Text RAG (via Ollama)

```python
from text_rag.ollama_embeddings import OllamaEmbedder

embedder = OllamaEmbedder(model="nomic-embed-text")
embedding = embedder.generate_embedding("Hello world")
print(f"Embedding shape: {len(embedding)}")  # 768 dims
```

### Vision RAG (via MLX-VLM)

```python
from vision_rag.mlx_vision_embedder import MLXVisionEmbedder

embedder = MLXVisionEmbedder(model_name="mlx-community/Qwen2-VL-2B-Instruct")
patch_embeddings = embedder.process_image("document_page.png")
print(f"Patch embeddings shape: {patch_embeddings.shape}")  # [1024, 128]
```

### Late Interaction Matching

```python
from vision_rag.late_interaction import LateInteractionMatcher

matcher = LateInteractionMatcher()
score = matcher.compute_similarity(query_embedding, document_patches)
print(f"Similarity score: {score}")
```

### Document Processing

```python
from vision_rag.document_processor import DocumentProcessor

processor = DocumentProcessor()
image_paths = processor.pdf_to_images("document.pdf", output_dir="./pages")
print(f"Generated {len(image_paths)} page images")
```

## ğŸ¨ ModÃ¨les supportÃ©s

### Vision RAG (MLX-VLM)

| ModÃ¨le | RAM Required | Vitesse | QualitÃ© |
|--------|-------------|---------|---------|
| `qwen2-vl-2b` | 16GB | âš¡âš¡âš¡ Rapide | â­â­ Bonne |
| `qwen2-vl-7b` | 32GB | âš¡âš¡ Moyen | â­â­â­ Excellente |
| `colpali-adapter` | 24GB | âš¡âš¡ Moyen | â­â­â­ Excellente |

### Text RAG (Ollama)

| ModÃ¨le | Dimensions | Taille | Performance |
|--------|-----------|--------|-------------|
| `nomic-embed-text` | 768 | 274MB | â­â­â­ Excellent |
| `mxbai-embed-large` | 1024 | 669MB | â­â­â­ TrÃ¨s bon |
| `all-minilm` | 384 | 23MB | â­â­ Bon |

## ğŸ§ª Tests

```bash
# Activer venv
source venv/bin/activate

# Run tests (quand disponibles)
pytest tests/

# Test manuel
python -m vision_rag.mlx_vision_embedder --test
```

## ğŸ› Troubleshooting

### Erreur: "MLX requires Apple Silicon"

MLX fonctionne uniquement sur puces Apple Silicon (M1/M2/M3/M4). Sur Intel Mac ou Linux, Vision RAG ne sera pas disponible. Text RAG continuera de fonctionner via Ollama.

### Erreur: "Out of memory"

Vision RAG avec Qwen2-VL-7B nÃ©cessite 32GB RAM. Solutions :
- Utiliser `qwen2-vl-2b` (16GB)
- RÃ©duire la rÃ©solution des images
- Traiter moins de pages simultanÃ©ment

### Erreur: "mlx_vlm not found"

```bash
pip install mlx-vlm --upgrade
```

### Performance lente

- VÃ©rifier que Metal est activÃ© (GPU Apple Silicon)
- Utiliser modÃ¨le plus lÃ©ger (qwen2-vl-2b)
- RÃ©duire la rÃ©solution (150 DPI au lieu de 200)

## ğŸ“š RÃ©fÃ©rences

- [MLX Framework](https://github.com/ml-explore/mlx)
- [MLX-VLM](https://github.com/Blaizzy/mlx-vlm)
- [ColPali Paper](https://arxiv.org/abs/2407.01449)
- [LanceDB](https://lancedb.github.io/lancedb/)
- [Qwen2-VL](https://huggingface.co/Qwen/Qwen2-VL)

## ğŸ¤ Support

Pour toute question sur l'environnement Python :
- Consulter cette documentation
- VÃ©rifier les logs : `python -m vision_rag.mlx_vision_embedder --debug`
- Ouvrir une issue GitHub

---

**Note** : Ce module Python est appelÃ© depuis Node.js via `python-shell`. Les services Node.js gÃ¨rent l'orchestration, ce module Python fournit uniquement les capacitÃ©s MLX/Vision RAG.
