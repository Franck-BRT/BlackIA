# Guide d'Impl√©mentation MLX Complet pour BlackIA

## üìã Vue d'Ensemble

Ce document d√©taille l'impl√©mentation compl√®te du syst√®me MLX dans BlackIA, permettant d'utiliser des LLM locaux optimis√©s pour Apple Silicon, avec un store de mod√®les int√©gr√© et t√©l√©chargement depuis Hugging Face.

## ‚úÖ Ce qui a √©t√© impl√©ment√©

### 1. Backend Python (Serveurs MLX)

#### `mlx_llm_server.py` - Serveur LLM principal
**Localisation:** `apps/desktop/src/main/services/backends/mlx/mlx_llm_server.py`

**Fonctionnalit√©s:**
- Chargement de mod√®les MLX depuis Hugging Face ou local
- Chat avec historique de messages (format ChatML)
- G√©n√©ration de texte
- Support streaming pour chat et g√©n√©ration
- D√©chargement de mod√®les pour lib√©rer la m√©moire
- Communication via stdin/stdout en JSON

**Commandes support√©es:**
```json
{
  "command": "load",
  "model_path": "mlx-community/Llama-3.2-3B-Instruct-4bit"
}

{
  "command": "chat",
  "messages": [{"role": "user", "content": "Hello"}],
  "max_tokens": 2048,
  "temperature": 0.7,
  "stream": true
}

{
  "command": "generate",
  "prompt": "Once upon a time",
  "max_tokens": 100
}

{
  "command": "unload"
}

{
  "command": "status"
}
```

#### `mlx_model_downloader.py` - T√©l√©chargeur de mod√®les
**Localisation:** `apps/desktop/src/main/services/backends/mlx/mlx_model_downloader.py`

**Fonctionnalit√©s:**
- T√©l√©chargement de mod√®les depuis Hugging Face
- Progression en temps r√©el
- Liste des mod√®les locaux
- Suppression de mod√®les
- Gestion automatique du cache

**Commandes support√©es:**
```json
{
  "command": "download",
  "repo_id": "mlx-community/Llama-3.2-3B-Instruct-4bit"
}

{
  "command": "list"
}

{
  "command": "delete",
  "model_path": "/path/to/model"
}
```

#### `mlx_embeddings.py` - Serveur d'embeddings (existant)
Conserv√© pour les embeddings RAG (sentence-transformers).

### 2. Backend TypeScript

#### `mlx-llm-backend.ts` - Backend LLM
**Localisation:** `apps/desktop/src/main/services/backends/mlx/mlx-llm-backend.ts`

**Classe:** `MLXLLMBackend extends BaseAIBackend`

**Capacit√©s:**
- `chat`: Chat conversationnel avec streaming
- `completion`: G√©n√©ration de texte
- `embeddings`: Embeddings (via mlx-backend.ts existant)

**M√©thodes principales:**
- `isAvailable()`: V√©rifie si mlx-lm est install√©
- `initialize()`: D√©marre le serveur Python
- `shutdown()`: Arr√™te le serveur
- `chat(request)`: Chat avec streaming
- `generate(request)`: G√©n√©ration de texte
- `loadModel(modelPath)`: Charge un mod√®le
- `unloadModel()`: D√©charge le mod√®le
- `listModels()`: Liste des mod√®les disponibles
- `getStatus()`: Statut du backend

#### `mlx-model-manager.ts` - Gestionnaire de mod√®les
**Localisation:** `apps/desktop/src/main/services/mlx-model-manager.ts`

**Classe:** `MLXModelManager extends EventEmitter`

**Fonctionnalit√©s:**
- Liste des mod√®les t√©l√©charg√©s localement
- T√©l√©chargement avec progression
- Suppression de mod√®les
- V√©rification de disponibilit√©
- M√©tadonn√©es (taille, type, repo ID)

**M√©thodes principales:**
- `initialize()`: Initialise le gestionnaire
- `listLocalModels()`: Liste des mod√®les locaux
- `downloadModel(repoId, onProgress)`: T√©l√©charge un mod√®le
- `deleteModel(modelPath)`: Supprime un mod√®le
- `isModelDownloaded(repoId)`: V√©rifie si t√©l√©charg√©
- `getModelPath(repoId)`: Obtient le chemin local

**√âv√©nements:**
- `download:progress`: Progression du t√©l√©chargement

#### `mlx-store-service.ts` - Store Hugging Face
**Localisation:** `apps/desktop/src/main/services/mlx-store-service.ts`

**Classe:** `MLXStoreService`

**Fonctionnalit√©s:**
- Recherche de mod√®les MLX sur Hugging Face
- Filtrage par tags, auteur, popularit√©
- Mod√®les recommand√©s pour BlackIA
- Cache des r√©sultats (1 heure)
- M√©tadonn√©es enrichies (taille, quantization, base model)

**M√©thodes principales:**
- `listAvailableModels(filters)`: Liste avec filtres
- `searchModels(query, limit)`: Recherche textuelle
- `getModelInfo(repoId)`: D√©tails d'un mod√®le
- `getRecommendedModels()`: Mod√®les recommand√©s
- `clearCache()`: Vide le cache

**Mod√®les recommand√©s:**
1. Llama-3.2-3B-Instruct-4bit (2GB) - Petit, rapide
2. Mistral-7B-Instruct-v0.3-4bit (4GB) - Qualit√© sup√©rieure
3. Qwen2.5-7B-Instruct-4bit (4GB) - Multilingue
4. Phi-3.5-mini-instruct-4bit (2.5GB) - Ultra compact
5. Meta-Llama-3.1-8B-Instruct-4bit (5GB) - Contexte long (131K tokens)

### 3. Handlers IPC

#### `mlx-handlers.ts` - Handlers complets
**Localisation:** `apps/desktop/src/main/mlx-handlers.ts`

**Handlers impl√©ment√©s:**

**Embeddings (existant):**
- `mlx:isAvailable`: V√©rifie disponibilit√©
- `mlx:getStatus`: Statut complet
- `mlx:listModels`: Mod√®les d'embeddings
- `mlx:getConfig`: Configuration
- `mlx:updateConfig`: Mise √† jour config
- `mlx:test`: Test connexion
- `mlx:restart`: Red√©marrage

**LLM (nouveau):**
- `mlx:llm:initialize`: Initialise le backend LLM
- `mlx:llm:loadModel`: Charge un mod√®le
- `mlx:llm:unloadModel`: D√©charge le mod√®le
- `mlx:llm:chat`: Chat avec streaming
- `mlx:llm:generate`: G√©n√©ration de texte
- `mlx:llm:getStatus`: Statut du LLM

**Gestion de mod√®les:**
- `mlx:models:initialize`: Initialise le gestionnaire
- `mlx:models:listLocal`: Liste des mod√®les locaux
- `mlx:models:download`: T√©l√©charge un mod√®le
- `mlx:models:delete`: Supprime un mod√®le
- `mlx:models:isDownloaded`: V√©rifie si t√©l√©charg√©

**Store:**
- `mlx:store:listAvailable`: Liste des mod√®les HF
- `mlx:store:search`: Recherche sur HF
- `mlx:store:getModelInfo`: D√©tails d'un mod√®le
- `mlx:store:getRecommended`: Mod√®les recommand√©s
- `mlx:store:clearCache`: Vide le cache

**√âv√©nements √©mis:**
- `mlx:llm:streamStart`: D√©but du streaming
- `mlx:llm:streamChunk`: Chunk de streaming
- `mlx:llm:streamEnd`: Fin du streaming
- `mlx:models:downloadProgress`: Progression t√©l√©chargement

### 4. Base de Donn√©es

#### Table `mlx_models`
**Localisation:** `apps/desktop/src/main/database/schema.ts`

**Sch√©ma:**
```typescript
{
  id: string;                // UUID
  repoId: string;           // "mlx-community/Llama-3.2-3B-Instruct-4bit"
  name: string;             // Nom convivial
  author: string;           // "mlx-community"

  // Stockage
  localPath: string;        // Chemin absolu
  size: number;             // Octets

  // M√©tadonn√©es
  modelType: 'chat' | 'completion' | 'embed';
  quantization: string;     // "4-bit", "8-bit"
  baseModel: string;        // "meta-llama/Llama-3.2-3B-Instruct"
  contextLength: number;    // 4096, 8192, etc.
  parameters: string;       // "3B", "7B"
  description: string;
  tags: string;            // JSON array

  // Utilisation
  downloaded: boolean;
  downloadedAt: Date;
  lastUsedAt: Date;
  usageCount: number;

  // Favoris
  isFavorite: boolean;
  isDefault: boolean;

  // Timestamps
  createdAt: Date;
  updatedAt: Date;
}
```

**Index:**
- `mlx_models_repo_id_idx` sur `repoId`
- `mlx_models_type_idx` sur `modelType`

## üöß Ce qu'il reste √† faire

### 1. Interface Utilisateur React

#### Composant `MLXModelStore.tsx`
**Localisation:** `apps/desktop/src/renderer/src/components/settings/MLXModelStore.tsx`

**Fonctionnalit√©s √† impl√©menter:**
- Liste des mod√®les disponibles sur Hugging Face
- Recherche avec filtres (auteur, tags, taille)
- Tri par t√©l√©chargements, likes, date
- Cards de mod√®les avec:
  - Nom, auteur, description
  - Taille, quantization, contexte
  - Tags, nombre de t√©l√©chargements
  - Bouton "Download" avec progress bar
  - Statut "Downloaded" si d√©j√† t√©l√©charg√©
- Onglets: Recommended, All Models, Search

**Exemple de structure:**
```tsx
export function MLXModelStore() {
  const [models, setModels] = useState([]);
  const [search, setSearch] = useState('');
  const [downloading, setDownloading] = useState<Record<string, number>>({});

  useEffect(() => {
    // Charger les mod√®les recommand√©s au d√©marrage
    loadRecommendedModels();
  }, []);

  const loadRecommendedModels = async () => {
    const result = await window.api.mlx.store.getRecommended();
    setModels(result.models);
  };

  const handleDownload = async (repoId: string) => {
    // √âcouter les √©v√©nements de progression
    window.api.mlx.models.onDownloadProgress((progress) => {
      if (progress.repoId === repoId) {
        setDownloading(prev => ({
          ...prev,
          [repoId]: progress.percentage
        }));
      }
    });

    // D√©marrer le t√©l√©chargement
    const result = await window.api.mlx.models.download(repoId);

    if (result.success) {
      // Rafra√Æchir la liste
      loadRecommendedModels();
    }
  };

  return (
    <div>
      <Tabs defaultValue="recommended">
        <TabsList>
          <TabsTrigger value="recommended">Recommended</TabsTrigger>
          <TabsTrigger value="all">All Models</TabsTrigger>
          <TabsTrigger value="search">Search</TabsTrigger>
        </TabsList>

        <TabsContent value="recommended">
          <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
            {models.map(model => (
              <ModelCard
                key={model.id}
                model={model}
                downloading={downloading[model.id]}
                onDownload={() => handleDownload(model.id)}
              />
            ))}
          </div>
        </TabsContent>
      </Tabs>
    </div>
  );
}
```

#### Composant `MLXModelManager.tsx`
**Localisation:** `apps/desktop/src/renderer/src/components/settings/MLXModelManager.tsx`

**Fonctionnalit√©s √† impl√©menter:**
- Liste des mod√®les t√©l√©charg√©s
- Informations d√©taill√©es (taille, type, date)
- Actions:
  - Set as default
  - Add to favorites
  - Test model
  - Delete
- Statistiques:
  - Nombre total de mod√®les
  - Espace disque utilis√©
  - Dernier mod√®le utilis√©

#### Mise √† jour `MLXSettings.tsx`
**Localisation:** `apps/desktop/src/renderer/src/components/settings/MLXSettings.tsx`

**Ajouter des onglets:**
- **General**: Configuration Python, mod√®les par d√©faut
- **Store**: `<MLXModelStore />`
- **Models**: `<MLXModelManager />`
- **Advanced**: Param√®tres avanc√©s (context length, etc.)

#### Int√©gration dans le Chat
**Fichiers √† modifier:**
- `apps/desktop/src/renderer/src/pages/Chat.tsx`

**Fonctionnalit√©s:**
- S√©lecteur de backend (Ollama / MLX)
- S√©lecteur de mod√®le MLX si backend MLX s√©lectionn√©
- Affichage du mod√®le actif
- Streaming des r√©ponses MLX

**Exemple:**
```tsx
const [backend, setBackend] = useState<'ollama' | 'mlx'>('ollama');
const [mlxModel, setMLXModel] = useState('');

const handleSendMessage = async (content: string) => {
  if (backend === 'mlx') {
    // Utiliser MLX
    await window.api.mlx.llm.chat({
      messages: [...messages, { role: 'user', content }],
      options: { max_tokens: 2048, temperature: 0.7 }
    });

    // √âcouter les chunks
    window.api.mlx.llm.onStreamChunk((data) => {
      // Ajouter le chunk au message
    });
  } else {
    // Utiliser Ollama (existant)
    // ...
  }
};
```

### 2. Preload API

**Fichier:** `apps/desktop/src/preload/index.ts`

**Ajouter les API MLX:**
```typescript
mlx: {
  // LLM
  llm: {
    initialize: () => ipcRenderer.invoke('mlx:llm:initialize'),
    loadModel: (modelPath: string) => ipcRenderer.invoke('mlx:llm:loadModel', modelPath),
    unloadModel: () => ipcRenderer.invoke('mlx:llm:unloadModel'),
    chat: (request: any) => ipcRenderer.invoke('mlx:llm:chat', request),
    generate: (request: any) => ipcRenderer.invoke('mlx:llm:generate', request),
    getStatus: () => ipcRenderer.invoke('mlx:llm:getStatus'),

    // √âv√©nements
    onStreamStart: (callback: (data: any) => void) => {
      ipcRenderer.on('mlx:llm:streamStart', (_event, data) => callback(data));
    },
    onStreamChunk: (callback: (data: any) => void) => {
      ipcRenderer.on('mlx:llm:streamChunk', (_event, data) => callback(data));
    },
    onStreamEnd: (callback: (data: any) => void) => {
      ipcRenderer.on('mlx:llm:streamEnd', (_event, data) => callback(data));
    },
  },

  // Gestion de mod√®les
  models: {
    initialize: () => ipcRenderer.invoke('mlx:models:initialize'),
    listLocal: () => ipcRenderer.invoke('mlx:models:listLocal'),
    download: (repoId: string) => ipcRenderer.invoke('mlx:models:download', repoId),
    delete: (modelPath: string) => ipcRenderer.invoke('mlx:models:delete', modelPath),
    isDownloaded: (repoId: string) => ipcRenderer.invoke('mlx:models:isDownloaded', repoId),

    // √âv√©nements
    onDownloadProgress: (callback: (progress: any) => void) => {
      ipcRenderer.on('mlx:models:downloadProgress', (_event, progress) => callback(progress));
    },
  },

  // Store
  store: {
    listAvailable: (filters?: any) => ipcRenderer.invoke('mlx:store:listAvailable', filters),
    search: (query: string, limit?: number) => ipcRenderer.invoke('mlx:store:search', query, limit),
    getModelInfo: (repoId: string) => ipcRenderer.invoke('mlx:store:getModelInfo', repoId),
    getRecommended: () => ipcRenderer.invoke('mlx:store:getRecommended'),
    clearCache: () => ipcRenderer.invoke('mlx:store:clearCache'),
  },

  // Embeddings (existant)
  isAvailable: () => ipcRenderer.invoke('mlx:isAvailable'),
  getStatus: () => ipcRenderer.invoke('mlx:getStatus'),
  // ... autres m√©thodes existantes
}
```

### 3. Syst√®me de Build - Embarquer Python

#### Cr√©er un bundle Python

**Option 1: PyInstaller (Recommand√©)**

Cr√©er `apps/desktop/scripts/build-python-bundle.sh`:
```bash
#!/bin/bash

echo "üêç Building Python bundle for MLX..."

# Cr√©er un environnement virtuel temporaire
python3 -m venv .venv-build
source .venv-build/bin/activate

# Installer les d√©pendances
pip install mlx-lm sentence-transformers huggingface_hub torch pyinstaller

# Cr√©er le bundle avec PyInstaller
pyinstaller --onedir \
  --name mlx-bundle \
  --hidden-import=mlx_lm \
  --hidden-import=sentence_transformers \
  --hidden-import=huggingface_hub \
  --collect-all mlx_lm \
  --collect-all sentence_transformers \
  apps/desktop/src/main/services/backends/mlx/mlx_llm_server.py

# Copier dans resources
cp -r dist/mlx-bundle resources/python/

# Nettoyer
deactivate
rm -rf .venv-build dist build

echo "‚úÖ Python bundle created in resources/python/"
```

**Option 2: Environnement virtuel relocatable**

Cr√©er `apps/desktop/scripts/create-python-env.sh`:
```bash
#!/bin/bash

echo "üêç Creating relocatable Python environment..."

# Cr√©er l'environnement
python3 -m venv resources/python-env --copies

# Activer
source resources/python-env/bin/activate

# Installer les d√©pendances
pip install mlx-lm sentence-transformers huggingface_hub torch

# D√©sactiver
deactivate

echo "‚úÖ Python environment created in resources/python-env/"
```

#### Mettre √† jour `electron-builder.yml`

```yaml
files:
  - dist
  - package.json
  - node_modules

  # Scripts Python
  - from: src/python
    to: python
    filter:
      - "**/*.py"
      - "**/*.md"
  - from: src/main/services/backends/mlx
    to: dist/main/services/backends/mlx
    filter:
      - "**/*.py"

  # Bundle Python (Option 1: PyInstaller)
  - from: resources/python/mlx-bundle
    to: resources/python

  # OU (Option 2: Environnement virtuel)
  - from: resources/python-env
    to: resources/python-env

asarUnpack:
  - "**/*.py"
  - "resources/python/**/*"
  # OU
  - "resources/python-env/**/*"
```

#### Mettre √† jour les chemins Python dans le code

**Fichiers √† modifier:**
- `mlx-llm-backend.ts`
- `mlx-model-manager.ts`
- `mlx-backend.ts`

**D√©tecter le bon Python:**
```typescript
private getPythonPath(): string {
  if (app.isPackaged) {
    // En production, utiliser le Python embarqu√©
    const resourcesPath = process.resourcesPath;

    // Option 1: PyInstaller bundle
    return join(resourcesPath, 'python', 'mlx-bundle', 'mlx_llm_server');

    // Option 2: Environnement virtuel
    return join(resourcesPath, 'python-env', 'bin', 'python3');
  } else {
    // En d√©veloppement, utiliser Python syst√®me
    return 'python3';
  }
}
```

### 4. Scripts de Build

#### Ajouter dans `package.json`

```json
{
  "scripts": {
    "build:python": "./scripts/build-python-bundle.sh",
    "build:dmg:mlx": "npm run build:python && npm run build:dmg",
    "build:full": "npm run build:python && npm run build:dmg:universal"
  }
}
```

### 5. Tests

#### Tests manuels

Cr√©er `MLX_TESTING_CHECKLIST.md`:

```markdown
# Checklist de tests MLX

## Installation
- [ ] Python 3.10+ install√©
- [ ] mlx-lm install√© (`pip install mlx-lm`)
- [ ] sentence-transformers install√©
- [ ] huggingface_hub install√©

## Backend LLM
- [ ] Serveur d√©marre correctement
- [ ] Ping r√©pond
- [ ] Status retourne les bonnes infos
- [ ] Mod√®le se charge sans erreur
- [ ] Chat streaming fonctionne
- [ ] G√©n√©ration fonctionne
- [ ] D√©chargement lib√®re la m√©moire

## Gestionnaire de mod√®les
- [ ] Liste les mod√®les locaux
- [ ] T√©l√©chargement avec progression
- [ ] Progression s'affiche correctement
- [ ] Mod√®le appara√Æt apr√®s t√©l√©chargement
- [ ] Suppression fonctionne

## Store
- [ ] Mod√®les recommand√©s s'affichent
- [ ] Recherche fonctionne
- [ ] Filtres fonctionnent
- [ ] M√©tadonn√©es correctes

## Interface utilisateur
- [ ] Store s'affiche correctement
- [ ] Bouton Download fonctionne
- [ ] Progress bar s'affiche
- [ ] Manager affiche les mod√®les
- [ ] Actions fonctionnent (delete, favorite)
- [ ] S√©lecteur de backend dans Chat
- [ ] Chat MLX fonctionne

## Build
- [ ] Bundle Python cr√©√©
- [ ] DMG contient Python
- [ ] Application fonctionne en production
- [ ] Pas d'erreur de chemin Python
```

## üì¶ D√©pendances Python requises

**Pour d√©veloppement:**
```bash
pip install mlx-lm sentence-transformers huggingface_hub torch
```

**Pour build (PyInstaller):**
```bash
pip install pyinstaller
```

**Versions test√©es:**
- Python: 3.10+
- mlx-lm: 0.1.0+
- sentence-transformers: 2.2.0+
- huggingface_hub: 0.20.0+
- torch: 2.1.0+

## üéØ Ordre d'impl√©mentation recommand√©

1. ‚úÖ **Backend Python** (Fait)
2. ‚úÖ **Backend TypeScript** (Fait)
3. ‚úÖ **Handlers IPC** (Fait)
4. ‚úÖ **Sch√©ma DB** (Fait)
5. üî≤ **Preload API** (√Ä faire)
6. üî≤ **Interface utilisateur** (√Ä faire)
   - MLXModelStore
   - MLXModelManager
   - MLXSettings mise √† jour
   - Int√©gration Chat
7. üî≤ **Syst√®me de build** (√Ä faire)
   - Script build Python
   - electron-builder config
   - D√©tection chemin Python
8. üî≤ **Tests** (√Ä faire)
9. üî≤ **Documentation utilisateur** (√Ä faire)

## üöÄ Utilisation rapide (une fois termin√©)

**Pour l'utilisateur:**
1. Ouvrir BlackIA
2. Aller dans Settings > MLX
3. Onglet "Store"
4. Choisir un mod√®le recommand√© (ex: Llama-3.2-3B-Instruct-4bit)
5. Cliquer "Download" et attendre
6. Aller dans Chat
7. S√©lectionner backend "MLX"
8. S√©lectionner le mod√®le t√©l√©charg√©
9. Commencer √† chatter !

**Avantages pour l'utilisateur:**
- 10-20x plus rapide qu'Ollama
- 100% local
- Optimis√© Apple Silicon
- Pas de serveur √† installer
- Store int√©gr√©
- Mod√®les quantifi√©s (petits et rapides)

## üìù Notes importantes

1. **Taille des mod√®les:**
   - 4-bit: ~50% de la taille originale
   - Llama 3B 4-bit: ~2GB
   - Llama 7B 4-bit: ~4GB
   - Llama 8B 4-bit: ~5GB

2. **M√©moire requise:**
   - 8GB RAM minimum
   - 16GB RAM recommand√©
   - Mod√®les 3-7B: ~4-8GB de m√©moire unifi√©e

3. **Performance:**
   - M1/M2/M3: ~20-40 tokens/sec (3B)
   - M1/M2/M3: ~10-20 tokens/sec (7B)
   - M1 Ultra: ~60-80 tokens/sec (7B)

4. **Compatibilit√©:**
   - macOS 13.0+ (Ventura)
   - Apple Silicon uniquement
   - Metal GPU requis

## üîó Ressources

- [MLX Documentation](https://ml-explore.github.io/mlx/)
- [mlx-lm GitHub](https://github.com/ml-explore/mlx-examples/tree/main/llms)
- [Hugging Face MLX Community](https://huggingface.co/mlx-community)
- [Mod√®les MLX recommand√©s](https://huggingface.co/collections/mlx-community/llama-32-6557c0e7e0b2d02fc2a04937)

## üÜò Support

En cas de probl√®me:
1. V√©rifier que Python et mlx-lm sont install√©s
2. V√©rifier les logs dans BlackIA
3. Tester les scripts Python manuellement
4. Consulter le README MLX
5. V√©rifier la m√©moire disponible

---

**Auteur:** Claude (Assistant IA)
**Date:** 2025-11-19
**Version:** 1.0.0
