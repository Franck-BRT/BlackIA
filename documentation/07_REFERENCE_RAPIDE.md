# BlackIA - Guide de RÃ©fÃ©rence Rapide

**Version**: 0.2.0
**Date**: Novembre 2025

---

## ğŸ¯ Raccourcis Clavier

### Navigation

| Raccourci | Action |
|-----------|--------|
| `Cmd + 1` | Ouvrir le Chat |
| `Cmd + 2` | Ouvrir les Workflows |
| `Cmd + 3` | Ouvrir les Personas |
| `Cmd + 4` | Ouvrir les Prompts |
| `Cmd + 5` | Ouvrir la Library |
| `Cmd + 6` | Ouvrir la Documentation |
| `Cmd + 7` | Ouvrir l'Editor |
| `Cmd + K` | Recherche globale |
| `Cmd + ,` | Ouvrir les ParamÃ¨tres |

### Actions globales

| Raccourci | Action |
|-----------|--------|
| `Cmd + N` | Nouvelle conversation / workflow |
| `Cmd + W` | Fermer l'onglet actuel |
| `Cmd + S` | Sauvegarder (workflow, editor) |
| `Cmd + Q` | Quitter l'application |
| `Cmd + R` | RafraÃ®chir la page |

### Chat

| Raccourci | Action |
|-----------|--------|
| `EntrÃ©e` | Envoyer le message |
| `Shift + EntrÃ©e` | Nouvelle ligne |
| `Cmd + Shift + C` | Copier le dernier message IA |
| `Cmd + Shift + E` | Exporter la conversation |
| `@` | Mention de persona |
| `/` | Insertion de prompt |

### Editor

| Raccourci | Action |
|-----------|--------|
| `Cmd + B` | Gras |
| `Cmd + I` | Italique |
| `Cmd + K` | InsÃ©rer un lien |
| `Cmd + Shift + C` | Code inline |
| `Cmd + Shift + K` | Bloc de code |
| `Cmd + Shift + G` | GÃ©nÃ©rer avec IA |
| `Cmd + /` | Commenter/dÃ©commenter |

---

## ğŸ’» Commandes CLI

### DÃ©veloppement

```bash
# Lancer en mode dÃ©veloppement
pnpm desktop:dev

# Build complet
pnpm build

# Tests
pnpm test
pnpm test:coverage

# Linting
pnpm lint
pnpm format

# VÃ©rification
pnpm verify
```

### Scripts personnalisÃ©s

```bash
# Development avec options
./scripts/dev.sh                # Lancement standard
./scripts/dev.sh --fresh        # Rebuild complet
./scripts/dev.sh --no-build     # Skip build (rapide)
./scripts/dev.sh --clean        # Nettoie avant

# Build DMG
./scripts/build-dmg.sh
./scripts/build-dmg.sh --clean
./scripts/build-dmg.sh --arch universal
./scripts/build-dmg.sh --sign

# VÃ©rification complÃ¨te
./scripts/verify-setup.sh

# Setup Python
./scripts/setup-python-venv.sh

# Nettoyage complet
./scripts/clean-reinstall.sh
```

### Ollama

```bash
# Lancer Ollama
ollama serve

# Lister les modÃ¨les
ollama list

# TÃ©lÃ©charger un modÃ¨le
ollama pull llama3.2:3b
ollama pull mistral:7b
ollama pull nomic-embed-text

# Tester un modÃ¨le
ollama run llama3.2:3b

# Supprimer un modÃ¨le
ollama rm llama3.2:3b

# VÃ©rifier l'API
curl http://localhost:11434/api/tags
```

### Python / RAG

```bash
# Activer l'environnement virtuel
source venv/bin/activate

# Installer les dÃ©pendances
pip install -r apps/desktop/src/python/requirements.txt

# Tester l'importation
python3 -c "import torch; print(torch.__version__)"
python3 -c "import sentence_transformers"
python3 -c "import lancedb"
python3 -c "from colpali_engine.models import ColPali"
```

---

## ğŸ“¡ API IPC (Handlers)

### Personas

```typescript
// GET
'personas:getAll' â†’ Persona[]
'personas:getById' â†’ Persona
'personas:search' â†’ Persona[]
'personas:filterByCategory' â†’ Persona[]
'personas:getFavorites' â†’ Persona[]

// WRITE
'personas:create' â†’ Persona
'personas:update' â†’ Persona
'personas:delete' â†’ void
'personas:toggleFavorite' â†’ Persona
'personas:incrementUsage' â†’ Persona

// BULK
'personas:import' â†’ Persona[]
'personas:export' â†’ void
```

### Prompts

```typescript
'prompts:getAll' â†’ Prompt[]
'prompts:getById' â†’ Prompt
'prompts:create' â†’ Prompt
'prompts:update' â†’ Prompt
'prompts:delete' â†’ void
'prompts:search' â†’ Prompt[]
```

### Workflows

```typescript
'workflows:getAll' â†’ Workflow[]
'workflows:getById' â†’ Workflow
'workflows:create' â†’ Workflow
'workflows:update' â†’ Workflow
'workflows:delete' â†’ void
'workflows:execute' â†’ ExecutionResult
'workflows:getVersions' â†’ WorkflowVersion[]
'workflows:createVersion' â†’ WorkflowVersion
'workflows:restoreVersion' â†’ Workflow
```

### Chat

```typescript
'conversations:getAll' â†’ Conversation[]
'conversations:create' â†’ Conversation
'conversations:update' â†’ Conversation
'conversations:delete' â†’ void
'messages:getByConversation' â†’ Message[]
'messages:create' â†’ Message
'chat:send' â†’ void (+ events 'chat:token')
```

### RAG / Library

```typescript
'libraries:getAll' â†’ Library[]
'libraries:create' â†’ Library
'library:addDocument' â†’ LibraryDocument
'library:indexDocument' â†’ IndexResult
'rag:search' â†’ RAGSearchResult[]
'attachments:upload' â†’ Attachment
'attachments:index' â†’ void
```

### Backends

```typescript
'backends:getStatus' â†’ BackendStatus[]
'backends:switch' â†’ void
'backends:getModels' â†’ Model[]
'ollama:pullModel' â†’ void (+ events 'ollama:pull-progress')
```

---

## ğŸ¨ Types TypeScript principaux

### Persona

```typescript
interface Persona {
  id: string;
  name: string;
  description: string;
  systemPrompt: string;
  model?: string | null;
  temperature?: number | null;
  maxTokens?: number | null;
  fewShots?: FewShotExample[] | null;
  avatar: string;
  color: 'purple' | 'blue' | 'pink' | 'green' | 'orange';
  category?: string | null;
  tags: string[];
  isDefault: boolean;
  isFavorite: boolean;
  usageCount: number;
  createdAt: Date;
  updatedAt: Date;
}

interface FewShotExample {
  id: string;
  userMessage: string;
  assistantResponse: string;
}
```

### Workflow

```typescript
interface Workflow {
  id: string;
  name: string;
  description?: string;
  nodes: WorkflowNode[];
  edges: WorkflowEdge[];
  group?: string | null;
  isFavorite: boolean;
  createdAt: Date;
  updatedAt: Date;
}

interface WorkflowNode {
  id: string;
  type: 'input' | 'output' | 'aiPrompt' | 'condition' | 'loop' | 'transform' | 'switch' | 'extract';
  position: { x: number; y: number };
  data: Record<string, unknown>;
}

interface WorkflowEdge {
  id: string;
  source: string;
  target: string;
  sourceHandle?: string | null;
  targetHandle?: string | null;
}
```

### RAG

```typescript
interface Attachment {
  id: string;
  conversationId?: string;
  filename: string;
  filepath: string;
  mimeType: string;
  filesize: number;
  ragMode: 'none' | 'text' | 'vision' | 'hybrid';
  isIndexedText: boolean;
  textEmbeddingModel?: string;
  textChunkCount?: number;
  isIndexedVision: boolean;
  visionEmbeddingModel?: string;
  visionPatchCount?: number;
  pageCount?: number;
  thumbnail?: string;
  metadata?: Record<string, unknown>;
  createdAt: Date;
  updatedAt: Date;
}

interface RAGSearchParams {
  query: string;
  libraryIds?: string[];
  mode: 'text' | 'vision' | 'hybrid';
  topK?: number;
  minScore?: number;
}

interface RAGSearchResult {
  documentId: string;
  chunkId: string;
  content: string;
  score: number;
  metadata: {
    filename: string;
    page?: number;
    chunkIndex?: number;
  };
}
```

---

## ğŸ“ Emplacements importants

### DonnÃ©es utilisateur (macOS)

```
~/Library/Application Support/BlackIA/
â”œâ”€â”€ database/
â”‚   â””â”€â”€ blackia.db                  # Base de donnÃ©es SQLite
â”œâ”€â”€ vector-store/                   # LanceDB (embeddings RAG)
â”‚   â”œâ”€â”€ text_embeddings/
â”‚   â””â”€â”€ vision_embeddings/
â”œâ”€â”€ libraries/                      # Documents utilisateur
â”‚   â””â”€â”€ [library-id]/
â”‚       â”œâ”€â”€ documents/
â”‚       â””â”€â”€ thumbnails/
â”œâ”€â”€ logs/                           # Logs applicatifs
â”‚   â”œâ”€â”€ main.log
â”‚   â”œâ”€â”€ renderer.log
â”‚   â””â”€â”€ python.log
â””â”€â”€ cache/                          # Cache temporaire
```

### Projet dÃ©veloppeur

```
/path/to/BlackIA/
â”œâ”€â”€ apps/desktop/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main/                   # Main process (Node.js)
â”‚   â”‚   â”œâ”€â”€ renderer/               # Frontend (React)
â”‚   â”‚   â”œâ”€â”€ python/                 # Services Python
â”‚   â”‚   â””â”€â”€ preload/                # Preload script
â”‚   â”œâ”€â”€ dist/                       # Build output
â”‚   â””â”€â”€ release/                    # DMG files
â”œâ”€â”€ packages/                       # Workspace packages
â”‚   â”œâ”€â”€ ollama/                     # Ollama client
â”‚   â”œâ”€â”€ shared/                     # Types partagÃ©s
â”‚   â””â”€â”€ ui/                         # Composants UI
â”œâ”€â”€ scripts/                        # Scripts de build
â””â”€â”€ documentation/                  # Cette doc
```

---

## ğŸ”§ Configuration rapide

### tsconfig.json (raccourci)

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "ESNext",
    "jsx": "react-jsx",
    "strict": true,
    "paths": {
      "@/*": ["./src/*"],
      "@blackia/shared": ["./packages/shared/src"]
    }
  }
}
```

### vite.config.ts (raccourci)

```typescript
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react';

export default defineConfig({
  plugins: [react()],
  base: './',
  server: { port: 5173 }
});
```

### electron-builder.yml (raccourci)

```yaml
appId: com.blackroom.blackia
productName: BlackIA
mac:
  category: public.app-category.productivity
  target: [dmg]
  icon: resources/icon.icns
```

---

## ğŸ› Debug rapide

### Logs en temps rÃ©el

```bash
# Main process
tail -f ~/Library/Application\ Support/BlackIA/logs/main.log

# Python
tail -f ~/Library/Application\ Support/BlackIA/logs/python.log

# Tous les erreurs
grep -r "ERROR" ~/Library/Application\ Support/BlackIA/logs/
```

### VÃ©rifications systÃ¨me

```bash
# VÃ©rifier les ports
lsof -i :5173    # Vite dev server
lsof -i :11434   # Ollama

# VÃ©rifier les processus
ps aux | grep -i electron
ps aux | grep -i python
ps aux | grep -i ollama

# Espace disque
du -h ~/Library/Application\ Support/BlackIA/database/blackia.db
du -sh ~/Library/Application\ Support/BlackIA/vector-store/
```

### Reset complet

```bash
# âš ï¸ ATTENTION : Supprime TOUTES les donnÃ©es

# ArrÃªter l'app d'abord !

# Supprimer les donnÃ©es
rm -rf ~/Library/Application\ Support/BlackIA/

# Au prochain lancement, tout est recrÃ©Ã©
```

---

## ğŸ“Š Benchmarks & Limites

### ModÃ¨les Ollama recommandÃ©s

| ModÃ¨le | Taille | RAM | Vitesse | Usage |
|--------|--------|-----|---------|-------|
| llama3.2:1b | 1 GB | 4 GB | âš¡âš¡âš¡âš¡âš¡ | Tests rapides |
| llama3.2:3b | 2 GB | 8 GB | âš¡âš¡âš¡âš¡ | Usage quotidien |
| mistral:7b | 4 GB | 16 GB | âš¡âš¡âš¡ | Ã‰quilibrÃ© |
| llama3.1:70b | 39 GB | 64 GB | âš¡ | Production |

### Performances RAG

| Mode RAG | Indexation | Recherche | RAM | PrÃ©cision |
|----------|------------|-----------|-----|-----------|
| Text | ~1s/page | ~50ms | 2 GB | â­â­â­ |
| Vision | ~5s/page | ~200ms | 8 GB | â­â­â­â­ |
| Hybrid | ~6s/page | ~250ms | 10 GB | â­â­â­â­â­ |

### Limites systÃ¨me

```
Maximum simultanÃ© :
â€¢ Conversations : IllimitÃ© (limitÃ© par DB size)
â€¢ Workflows : IllimitÃ©
â€¢ Documents RAG : ~10,000 (performance optimale)
â€¢ Chunks par document : ~1,000
â€¢ Taille document : 100 MB (recommandÃ© < 10 MB)

Context window (dÃ©pend du modÃ¨le) :
â€¢ llama3.2:3b : 8,192 tokens (~6,000 mots)
â€¢ mistral:7b : 32,768 tokens (~24,000 mots)
â€¢ llama3.1:70b : 128,000 tokens (~96,000 mots)
```

---

## ğŸŒ URLs et Endpoints

### Ollama API

```
Base URL : http://localhost:11434

Endpoints :
POST /api/chat         # Chat avec streaming
POST /api/generate     # GÃ©nÃ©ration texte
POST /api/embeddings   # Embeddings
GET  /api/tags         # Liste modÃ¨les
POST /api/pull         # TÃ©lÃ©charger modÃ¨le
GET  /api/version      # Version Ollama
```

### MLX (Apple Silicon)

```
Local Python module import :
from mlx_lm import load, generate

Pas d'API HTTP (import direct)
```

---

## ğŸ“ Snippets de code courants

### Appeler l'IPC depuis React

```typescript
// GET data
const personas = await window.electron.ipcRenderer.invoke('personas:getAll');

// POST data
const newPersona = await window.electron.ipcRenderer.invoke(
  'personas:create',
  {
    name: 'Expert Docker',
    description: '...',
    systemPrompt: '...',
    // ...
  }
);

// Ã‰couter un Ã©vÃ©nement
useEffect(() => {
  const unsubscribe = window.electron.ipcRenderer.on(
    'chat:token',
    (token) => {
      setMessage(prev => prev + token);
    }
  );

  return () => unsubscribe();
}, []);
```

### CrÃ©er un service

```typescript
// apps/desktop/src/main/services/my-service.ts
export class MyService {
  async getAll() {
    const results = await db.select().from(myTable);
    return results;
  }

  async create(data: CreateInput) {
    const newItem = {
      id: randomUUID(),
      ...data,
      createdAt: new Date(),
    };
    await db.insert(myTable).values(newItem);
    return newItem;
  }
}

export const myService = new MyService();
```

### Enregistrer un handler IPC

```typescript
// apps/desktop/src/main/handlers/my-handlers.ts
import { ipcMain } from 'electron';
import { myService } from '../services/my-service';

export function registerMyHandlers() {
  ipcMain.handle('my:getAll', async () => {
    return await myService.getAll();
  });

  ipcMain.handle('my:create', async (_event, data) => {
    return await myService.create(data);
  });
}

// Dans main/index.ts
import { registerMyHandlers } from './handlers/my-handlers';

app.whenReady().then(() => {
  registerMyHandlers();
  // ... autres handlers
});
```

### Hook React personnalisÃ©

```typescript
// apps/desktop/src/renderer/src/hooks/useMyData.ts
import { useQuery, useMutation, useQueryClient } from '@tanstack/react-query';

export function useMyData() {
  return useQuery({
    queryKey: ['myData'],
    queryFn: async () => {
      return await window.electron.ipcRenderer.invoke('my:getAll');
    },
  });
}

export function useCreateMyData() {
  const queryClient = useQueryClient();

  return useMutation({
    mutationFn: async (data: CreateInput) => {
      return await window.electron.ipcRenderer.invoke('my:create', data);
    },
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ['myData'] });
    },
  });
}
```

---

**Fin du Guide de RÃ©fÃ©rence Rapide**

*Pour plus de dÃ©tails, consultez les manuels complets.*

*DerniÃ¨re mise Ã  jour: Novembre 2025*
*Version du document: 1.0*
